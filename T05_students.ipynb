{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqrn-bHiMq7W"
      },
      "source": [
        "---\n",
        "Gradient Descent\n",
        "---\n",
        "**Input**:\n",
        "- Dataset $\\mathcal{D}=\\{(\\mathbf{x}^{(n)}, y^{(n)})\\}_{n=1}^N$\n",
        "- Objective $J_\\mathcal{D}(\\mathbf{w})$\n",
        "- Gradient $\\nabla_\\mathbf{w} J_\\mathcal{D}(\\mathbf{w})$\n",
        "- Learning rate $\\eta>0$\n",
        "- Convergence threshold $\\epsilon>0$. \\\\\n",
        "\n",
        "*(assuming $\\mathbf{w}$ contains all parameters)*\n",
        "\n",
        "$\\mathbf{w}^{(0)} \\leftarrow \\mathbf{w}_\\text{init}$. *(Initialize weights)* \\\\\n",
        "$t \\leftarrow 0$\n",
        "\n",
        "**repeat**\n",
        "> $t \\leftarrow t+1$ \\\\\n",
        "> $\\mathbf{w}^{(t)} \\leftarrow \\mathbf{w}^{(t-1)} - \\eta \\nabla J_\\mathcal{D}(\\mathbf{w}^{(t-1)})$. *(Update weights)* \\\\\n",
        "\n",
        "**until** $|J_\\mathcal{D}(\\mathbf{w}^{(t)})-J_\\mathcal{D}(\\mathbf{w}^{(t-1)})|<\\epsilon  $ \\\\\n",
        "\n",
        "**return** $\\mathbf{w}^{(t)}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPU0_tDAhwd7"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeXaf7BlgKcx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "def plot_1D_data(pos,neg,xlim=(-5,5),ylim=(-5,5)):\n",
        "  plt.grid(color=[.9,.9,.9])\n",
        "  plt.scatter(pos,np.zeros(pos.size),s=100,c='b',marker='+')\n",
        "  plt.scatter(neg,np.zeros(pos.size),s=100,c='r',marker='_')\n",
        "  plt.xlim(xlim)\n",
        "  plt.ylim(ylim)\n",
        "  plt.xlabel('$x_1$')\n",
        "\n",
        "def plot_2D_data(pos,neg,xlim=(-5,5),ylim=(-5,5)):\n",
        "  plt.grid(color=[.9,.9,.9])\n",
        "  plt.scatter(pos[:,0],pos[:,1],s=100,c='b',marker='+')\n",
        "  plt.scatter(neg[:,0],neg[:,1],s=100,c='r',marker='_')\n",
        "  plt.xlim(xlim)\n",
        "  plt.ylim(ylim)\n",
        "  plt.xlabel('$x_1$')\n",
        "  plt.ylabel('$x_2$')\n",
        "\n",
        "def plot_2D_plane(w,w0,xlim,ylim):\n",
        "  xx = np.linspace(xlim[0],xlim[1],100)\n",
        "  l1 = plt.plot(xx, -w[0]/w[1]*xx -w0/w[1], label='hyperplane')\n",
        "  plt.grid(color=[.9,.9,.9])\n",
        "  d = -w0/np.linalg.norm(w)\n",
        "  o = w/np.linalg.norm(w) * -w0/np.linalg.norm(w);\n",
        "  xx2 = np.linspace(0,o[0],10)\n",
        "  yy2 = np.linspace(0,o[1],10)\n",
        "  label = \"$d=-w_0/|\\mathbf{w}|=%.2f$\" % d\n",
        "  l2 = plt.plot(xx2,yy2,'--',label=label)\n",
        "  l3 = plt.arrow(o[0], o[1], w[0], w[1], head_width=.1, label='$\\mathbf{w}$')\n",
        "  plt.xlabel('$x_1$')\n",
        "  plt.ylabel('$x_2$')\n",
        "  plt.axis('square')\n",
        "  plt.xlim(xlim)\n",
        "  plt.ylim(ylim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00c_N3r0hnyr"
      },
      "source": [
        "# Old Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbKiPIh5giRn"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    An implementation for the original perceptron, a linear classification\n",
        "    algorithm for binary classification tasks.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    num_epochs: int\n",
        "        The number of epochs (iterations) to run the perceptron algorithm for.\n",
        "    weights: np.ndarray, shape depends on the dataset\n",
        "        The weight vector for the perceptron. Initialized to zeros.\n",
        "    training_error: np.ndarray, shape (num_epochs,)\n",
        "        Contains the number of mistakes made at every epoch.\n",
        "    weights_history: np.ndarray, shape depends on the dataset\n",
        "        Contains the weights computed at every epoch.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the perceptron to the training data (X, y) using the perceptron\n",
        "        algorithm. See method for a detailed description.\n",
        "\n",
        "    predict(X):\n",
        "        Predicts the class labels for the input data X using the current weight\n",
        "        vector. See method for a detailed description.\n",
        "\n",
        "    plot_training_error():\n",
        "        Plot the training error. The x-axis corresponds to the epoch, the y-axis\n",
        "        corresponds to the number of mistakes made within an epoch.\n",
        "\n",
        "    plot_decision_boundary(X, y, weights_to_plot):\n",
        "        Plot the dataset (X, y) and the decision boundary of the classifier with\n",
        "        weights from specific epochs indicated by `weights_to_plot`. See method\n",
        "        for a detailed description.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_epochs=10):\n",
        "        \"\"\"\n",
        "        Initializes a new Perceptron instance.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        num_epochs: int, optional (default=10)\n",
        "            The number of epochs to run the perceptron algorithm for.\n",
        "        \"\"\"\n",
        "        self.num_epochs = num_epochs\n",
        "        self.training_error = np.zeros(num_epochs)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the perceptron to the training data (X, y) using the perceptron\n",
        "        algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X: np.ndarray, shape (num_samples, num_features)\n",
        "            The training data matrix, where each row is an input vector and each\n",
        "            column is a feature.\n",
        "        y: np.ndarray, shape (num_samples,)\n",
        "            The class label vector for the training data. Each element\n",
        "            corresponds to the class label for the corresponding row in X.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self: Perceptron\n",
        "            The instance that was trained on the dataset (X, y).\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # For simplicity, add a column of ones to the features to include the\n",
        "        # bias in the weight vector\n",
        "        column_ones = np.ones((num_samples, 1))\n",
        "        X_aug = np.hstack((column_ones, X))\n",
        "\n",
        "        # Initialize weight vector to zeros\n",
        "        self.weights = np.zeros(num_features + 1)\n",
        "        self.weights_history = np.zeros((self.num_epochs, num_features + 1))\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "\n",
        "            # Log the weights of the current epoch\n",
        "            self.weights_history[epoch, :] = self.weights\n",
        "\n",
        "            count_errors = 0\n",
        "            for i in range(num_samples):\n",
        "                x = X_aug[i, :]\n",
        "                y_true = y[i]\n",
        "\n",
        "                # Calculate dot product of weights and input vector\n",
        "                z = np.dot(self.weights, x)\n",
        "\n",
        "                # Update if there is a mistake\n",
        "                if y[i] * z <= 0:\n",
        "                  count_errors += 1\n",
        "                  self.weights += y[i] * X_aug[i, :]\n",
        "\n",
        "            # Log the errors of the current epoch\n",
        "            self.training_error[epoch] = count_errors\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the class labels for the input data X using the current weight\n",
        "        vector.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X: numpy.ndarray of shape (num_samples, num_features)\n",
        "            The input data matrix, where each row is an input vector and each\n",
        "            column is a feature.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        y_pred: numpy.ndarray of shape (num_samples,)\n",
        "            The predicted class labels for the input data, where each element is\n",
        "            either 1 or -1.\n",
        "        \"\"\"\n",
        "        num_samples = X.shape[0]\n",
        "        y_pred = np.zeros(num_samples)\n",
        "\n",
        "        # Add a column of ones for the bias\n",
        "        column_ones = np.ones((num_samples, 1))\n",
        "        X_aug = np.hstack((column_ones, X))\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Calculate dot product of weights and input vector\n",
        "            z = np.dot(self.weights, X_aug[i, :])\n",
        "\n",
        "            # Apply step function to activation to get predicted class\n",
        "            y_pred[i] = np.where(z >= 0, 1, -1)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def plot_training_error(self):\n",
        "        \"\"\"\n",
        "        Plot the training error. The x-axis corresponds to the epoch, the y-axis\n",
        "        corresponds to the number of mistakes made within an epoch.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(5, 3))\n",
        "\n",
        "        plt.title('Training error')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Number of errors')\n",
        "\n",
        "        plt.plot(self.training_error)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def plot_decision_boundary(self, X, y, weights_to_plot=None):\n",
        "        \"\"\"\n",
        "        Plot the training error. The x-axis corresponds to the epoch, the y-axis\n",
        "        corresponds to the number of mistakes made within an epoch.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X: np.ndarray, shape (num_samples, num_features)\n",
        "            An input data matrix, where each row is an input vector and each\n",
        "            column is a feature.\n",
        "\n",
        "        y: np.ndarray, shape (num_samples,)\n",
        "            The class label vector for the training data. Each element\n",
        "            corresponds to the class label for the corresponding row in X.\n",
        "\n",
        "        weights_to_plot: np.ndarray, shape (num_weights,)\n",
        "            A list of epochs indices, to indicate which epochs to plot.\n",
        "        \"\"\"\n",
        "        if weights_to_plot is None:\n",
        "          weights_to_plot = np.array([-1])  # Plot the last decision boundary\n",
        "\n",
        "        num_weights = len(weights_to_plot)\n",
        "\n",
        "        plt.figure()\n",
        "        colormap = ListedColormap(np.array(plt.cm.Paired.colors)[[1, 5]])\n",
        "\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colormap, edgecolors='k', linewidth=.5)\n",
        "        plt.scatter([], [], color=colormap.colors[0], edgecolors='k', linewidth=.5, label='Class -1')\n",
        "        plt.scatter([], [], color=colormap.colors[1], edgecolors='k', linewidth=.5, label='Class 1')\n",
        "\n",
        "        xlim = plt.xlim()\n",
        "        ylim = plt.ylim()\n",
        "\n",
        "        xmin = xlim[0] - 0.1 * abs(xlim[0])\n",
        "        xmax = xlim[1] + 0.1 * abs(xlim[1])\n",
        "\n",
        "        ymin = ylim[0] - 0.1 * abs(ylim[0])\n",
        "        ymax = ylim[1] + 0.1 * abs(ylim[1])\n",
        "\n",
        "        xx = np.linspace(xmin, xmax, 10)\n",
        "\n",
        "        viridis = plt.colormaps['viridis']\n",
        "        colors = [viridis(i) for i in np.linspace(0, 1, num_weights)]\n",
        "\n",
        "        for i, j in enumerate(weights_to_plot):\n",
        "            weights = self.weights_history[j, :]\n",
        "            if weights[2] != 0:\n",
        "                yy = - (weights[1] * xx + weights[0]) / weights[2]\n",
        "                plt.plot(xx, yy, c=colors[i], label=f'Epoch {j}')\n",
        "            elif weights[1] != 0:\n",
        "                plt.axvline(x = - weights[0] / weights[1], color=colors[i], label=f'Epoch {j}')\n",
        "\n",
        "        plt.xlim((xmin, xmax))\n",
        "        plt.ylim((ymin, ymax))\n",
        "        plt.title('Decision boundary')\n",
        "        plt.xlabel('x_1')\n",
        "        plt.ylabel('x_2')\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEZe1XPyJsu"
      },
      "source": [
        "# LR and Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVJ2GIDEsLZE"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "class my_LogisticRegression():\n",
        "    \"\"\"\n",
        "    Logistic Regression implemented with batch Gradient Descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lam = 1e-15):\n",
        "        self.n_features = 0\n",
        "        self.n_samples = 0\n",
        "\n",
        "        # parameters for the decision rule\n",
        "        self.w = np.zeros([self.n_features+1]) # the intercept is included in w\n",
        "        self.nb_iter = 1\n",
        "\n",
        "        self.lam = lam\n",
        "        self.training_error = np.array([])\n",
        "\n",
        "    def cross_entropy_error(self, X, y, w):\n",
        "        \"\"\"\n",
        "        Computes the Cross-Entropy Loss Function\n",
        "        Assumes an extra input dimension to account for the bias.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "\n",
        "        w: ndarray, shape(1, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        error: integer,\n",
        "                The Cross-Entropy Loss\n",
        "        \"\"\"\n",
        "\n",
        "        loglik = 0\n",
        "        for n in range(X.shape[0]):\n",
        "            yhat = sigmoid(w@X[n,:])\n",
        "\n",
        "            # TODO\n",
        "            # compute negative log-likelihood / cross-entropy error for this point\n",
        "            loglik_n = y[n]*np.log(yhat+1e-9)+(1-y[n])*np.log(1-yhat+1e-9)\n",
        "            # Note: the 1e-9 is added to avoid numerical errors\n",
        "\n",
        "            loglik += loglik_n\n",
        "\n",
        "        return -loglik/X.shape[0] + self.lam * np.power(np.linalg.norm(w),2)/2\n",
        "\n",
        "    def gradient(self, X, y, w):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the Cross-Entropy Loss Function\n",
        "        Assumes an extra input dimension to account for the bias.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "\n",
        "        w: ndarray, shape(1, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        grad: ndarray, shape (1, n_features)\n",
        "                The gradient of the Cross-Entropy Loss Function\n",
        "        \"\"\"\n",
        "\n",
        "        #initialize gradient\n",
        "        grad = np.zeros([self.n_features+1])\n",
        "\n",
        "        for n in range(X.shape[0]):\n",
        "            yhat = sigmoid(w@X[n,:])\n",
        "\n",
        "            gradient_n = (yhat-y[n])*X[n,:]\n",
        "            grad += gradient_n\n",
        "\n",
        "        return grad/X.shape[0] + self.lam*w\n",
        "\n",
        "    def fit(self, X, y, max_iter = 10000, eps = 1E-7, learning_rate = 1E-6):\n",
        "        \"\"\"\n",
        "        Fit our Logistic Regression model to the data set (X, y)\n",
        "        ADDS an extra input dimension to account for the bias.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "        Returns\n",
        "        -------\n",
        "        self: logReg\n",
        "               The Logistic Regression model trained on (X, y).\n",
        "        \"\"\"\n",
        "\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.n_features = X.shape[1]\n",
        "        X = np.hstack([np.ones((self.n_samples,1)),X])  # add dummy input\n",
        "        self.w = np.zeros([1, self.n_features + 1])[0]\n",
        "\n",
        "        convergence = False  # start with cond greater than eps\n",
        "        prev_error = np.Inf\n",
        "        while not convergence and self.nb_iter < max_iter:\n",
        "            w_old = self.w\n",
        "\n",
        "            #update weights\n",
        "            self.w = w_old - learning_rate*self.gradient(X, y, self.w)\n",
        "\n",
        "            #compute cross entropy error for this iteration and save it\n",
        "            error = self.cross_entropy_error(X, y, self.w)\n",
        "\n",
        "            self.training_error = np.append(self.training_error, error)\n",
        "            self.nb_iter += 1\n",
        "\n",
        "            #check convergence condition\n",
        "            convergence = abs(error - prev_error) < eps\n",
        "            prev_error = error\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of points in X.\n",
        "        ADDS an extra input dimension to account for the bias.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The set of points whose classes are to predict.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions: ndarray, shape (n_samples)\n",
        "                      The predicted classes.\n",
        "                      n_samples == number of points in the dataset.\n",
        "        \"\"\"\n",
        "        X = np.hstack([np.ones((X.shape[0],1)),X])\n",
        "\n",
        "        # return prediction of the model for given X data points\n",
        "        prediction = np.reshape(sigmoid(self.w@X.T) > 0.5,(-1,))\n",
        "        return prediction\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the classifier on the set X, provided the ground-truth y.\n",
        "        Assumes an extra input dimension to account for the bias.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The set of points on which to compute the score.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The ground-truth of the labels.\n",
        "            n_samples == number of points in the dataset.\n",
        "        Returns\n",
        "        -------\n",
        "        score: float\n",
        "                Accuracy of the classifier.\n",
        "        \"\"\"\n",
        "        if self.training_error.size == 0:\n",
        "          print('train first!')\n",
        "          return -1\n",
        "        else:\n",
        "          return np.mean(self.predict(X) == y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzJjga7tyDbH"
      },
      "source": [
        "# Example 1: Sigmoid 1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj4V5jBfgYkP"
      },
      "outputs": [],
      "source": [
        "pos = np.array([2,3])\n",
        "neg = np.array([0,1])\n",
        "\n",
        "X = np.expand_dims(np.hstack([pos,neg]),1)\n",
        "y = np.array([1,1,0,0])\n",
        "\n",
        "xx = np.linspace(-1,5,50)\n",
        "x2 = np.expand_dims(xx,1)\n",
        "o = np.expand_dims(np.ones((50)),1)\n",
        "x2 = np.hstack([o,x2])\n",
        "\n",
        "# first iterations\n",
        "max_iter =10\n",
        "clf = my_LogisticRegression()\n",
        "clf.fit(X, y, learning_rate =10, eps = 1e-4, max_iter = max_iter)\n",
        "print(clf.w)\n",
        "print(f'The accuracy of the logistic regression on the training set is {clf.score(X,y)}')\n",
        "plot_1D_data(pos,neg,xlim=(-1,5),ylim=(-0.1,1))\n",
        "plt.title(f'Iteration: {max_iter}')\n",
        "plt.plot(xx,sigmoid(np.dot(x2,clf.w)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MilDWvT1jC4"
      },
      "outputs": [],
      "source": [
        "# show progress\n",
        "for max_iter in range(1,10000,500) :\n",
        "  clf = my_LogisticRegression()\n",
        "  clf.fit(X, y, learning_rate = 10, eps = 1e-5, max_iter = max_iter)\n",
        "  print(clf.w)\n",
        "  print(f'The accuracy of the logistic regression on the training set is {clf.score(X,y)}')\n",
        "  plot_1D_data(pos,neg,xlim=(-1,5),ylim=(-0.1,1))\n",
        "  plt.title(f'Iteration: {max_iter}')\n",
        "  plt.plot(xx,sigmoid(np.dot(x2,clf.w)))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj6YHACl5U3C"
      },
      "outputs": [],
      "source": [
        "# show error\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error, label='error no shift')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.grid(color=[.9,.9,.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8ovwhfmVQRV"
      },
      "source": [
        "# Example 2: Perceptron vs Logistic LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv1qkk1uVLqB"
      },
      "outputs": [],
      "source": [
        "pos = np.array([[6,4],[4,6],[6,6]])\n",
        "neg = np.array([[1,1]])\n",
        "xlim = (-3,3)\n",
        "ylim = (-3,3)\n",
        "X = np.vstack([pos,neg])\n",
        "npos = pos.shape[0]\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "y = np.array([1,1,1,0])\n",
        "print(X)\n",
        "plot_2D_data(X[:npos,:],X[npos:,:],xlim=xlim,ylim=ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxLjRCmseflw"
      },
      "outputs": [],
      "source": [
        "clf = Perceptron(num_epochs=5)\n",
        "y_perc = np.copy(y)\n",
        "y_perc[y==0] = -1\n",
        "clf.fit(X, y_perc)\n",
        "print(f\"Weights of the classifier: {clf.weights}\")\n",
        "w = clf.weights\n",
        "\n",
        "# Plot the training error\n",
        "y_pred = clf.predict(X)\n",
        "clf.plot_training_error()\n",
        "\n",
        "plot_2D_data(X[:npos,:],X[npos:,:],xlim=xlim,ylim=ylim)\n",
        "plot_2D_plane(w[1:],w[0],xlim,ylim)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYUYVsNkVhTv"
      },
      "outputs": [],
      "source": [
        "# show progress\n",
        "for max_iter in range(2,250,2) :\n",
        "  clf = my_LogisticRegression()\n",
        "  clf.fit(X, y, learning_rate = .05, eps = 1e-8, max_iter = max_iter)\n",
        "  print(f'The accuracy of the logistic regression on the training set is {clf.score(X,y)}')\n",
        "  plt.title(f'Iteration: {max_iter}')\n",
        "  plot_2D_data(X[:npos,:],X[npos:,:],xlim=xlim,ylim=ylim)\n",
        "  plot_2D_plane(clf.w[1:],clf.w[0],xlim,ylim)\n",
        "  plt.show()\n",
        "  print(f'w = {clf.w}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0efd-6WoVtuu"
      },
      "outputs": [],
      "source": [
        "# show error\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error,label='Error')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.grid(color=[.9,.9,.9])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUPJXZ7h8A-f"
      },
      "source": [
        "# Example 3: Sigmoid 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QnwDZQy7RKg"
      },
      "outputs": [],
      "source": [
        "pos = np.array([[2,-1],[0,1],[-2,3]])\n",
        "neg = np.array([[2,-3],[0,-1],[-2,1]])\n",
        "xlim = (-4,4)\n",
        "ylim = (-4,4)\n",
        "\n",
        "X = np.vstack([pos,neg])\n",
        "print(np.mean(X,axis=0))\n",
        "y = np.array([1,1,1,0,0,0])\n",
        "plot_2D_data(pos,neg,xlim=xlim,ylim=ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikUdLP9t9ASs"
      },
      "outputs": [],
      "source": [
        "# first iterations\n",
        "max_iter = 2\n",
        "clf = my_LogisticRegression()\n",
        "clf.fit(X, y, learning_rate = 0.1, eps = 1e-4, max_iter = max_iter)\n",
        "print(f'The accuracy of the logistic regression on the training set is {clf.score(X,y)}')\n",
        "plt.title(f'Iteration: {max_iter}')\n",
        "plot_2D_data(pos,neg,xlim,ylim)\n",
        "plot_2D_plane(clf.w[1:],clf.w[0],xlim,ylim)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP2xUMIE97Kq"
      },
      "outputs": [],
      "source": [
        "# show progress\n",
        "for max_iter in range(2,500,50) :\n",
        "  clf = my_LogisticRegression()\n",
        "  clf.fit(X, y, learning_rate = 0.5, eps = 1e-5, max_iter = max_iter)\n",
        "  print(f'The accuracy of the logistic regression on the training set is {clf.score(X,y)}')\n",
        "  plt.title(f'Iteration: {max_iter}')\n",
        "  plot_2D_data(pos,neg,xlim,ylim)\n",
        "  plot_2D_plane(clf.w[1:],clf.w[0],xlim,ylim)\n",
        "  plt.show()\n",
        "  print(f'w = {clf.w}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcDKCDPiHXYP"
      },
      "outputs": [],
      "source": [
        "# show error\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.grid(color=[.9,.9,.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3io-_ULcRs"
      },
      "source": [
        "# Example 4: Sigmoid 2D\n",
        "\n",
        "### Importance of standardizing data\n",
        "\n",
        "Shifting the data causes the problem to be harder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDC3VnCnKDN2"
      },
      "outputs": [],
      "source": [
        "pos_shift = np.array([[2,-1],[0,1],[-2,3]]) + 2\n",
        "neg_shift = np.array([[2,-3],[0,-1],[-2,1]]) + 2\n",
        "xlim_shift = (-2,6)\n",
        "ylim_shift = (-2,6)\n",
        "\n",
        "X_shift = np.vstack([pos_shift,neg_shift])\n",
        "y = np.array([1,1,1,0,0,0])\n",
        "plot_2D_data(pos_shift,neg_shift,xlim=xlim_shift,ylim=ylim_shift)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sqV-WeNLiNz"
      },
      "outputs": [],
      "source": [
        "# show progress\n",
        "for max_iter in range(2,5000,500) :\n",
        "  clf2 = my_LogisticRegression()\n",
        "  clf2.fit(X_shift, y, learning_rate = 1, eps = 1e-5, max_iter = max_iter)\n",
        "  print(f'The accuracy of the logistic regression on the training set is {clf.score(X_shift,y)}')\n",
        "  plt.title(f'Iteration: {max_iter}')\n",
        "  plot_2D_data(pos_shift,neg_shift,xlim_shift,ylim_shift)\n",
        "  plot_2D_plane(clf2.w[1:],clf2.w[0],xlim_shift,ylim_shift)\n",
        "  plt.show()\n",
        "  print(f'w = {clf2.w}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r88SkhCULsI5"
      },
      "outputs": [],
      "source": [
        "# show error\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error,label='centered')\n",
        "plt.plot(clf2.training_error,label='shifted')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.grid(color=[.9,.9,.9])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK6bWqGRxcXR"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "Using the one-dimensional problem, the error surface for $\\lambda>0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGuKeDRzxXMS"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from matplotlib import cm\n",
        "\n",
        "pos = np.array([2,3])\n",
        "neg = np.array([0,1])\n",
        "X = np.expand_dims(np.hstack([pos,neg]),1)\n",
        "X = X  + 0\n",
        "y = np.array([1,1,0,0])\n",
        "\n",
        "for lam in np.logspace(-4,-1,4):\n",
        "  # Make data\n",
        "  clf = my_LogisticRegression(lam = lam)\n",
        "  bins = 100\n",
        "  w0 = np.linspace(-50,50,bins)\n",
        "  w1 = np.linspace(-50,50,bins)\n",
        "  W0, W1 = np.meshgrid(w0, w1)\n",
        "  J = np.zeros((bins,bins))\n",
        "  i = 0\n",
        "  j = 0\n",
        "  for i in range(W0.shape[0]):\n",
        "    for j in range(W0.shape[1]):\n",
        "      w0 = W0[i,j]\n",
        "      w1 = W1[i,j]\n",
        "      J[i,j] = clf.cross_entropy_error(np.hstack([np.ones((4,1)),X]), y, np.array([w0,w1]))\n",
        "\n",
        "  plt.subplots()\n",
        "  plt.title(f'With $\\\\lambda=$ {lam}')\n",
        "  plt.xlabel('$w_0$')\n",
        "  plt.ylabel('$w_1$')\n",
        "  plt.contour(W0,W1,J)\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "  surf = ax.plot_surface(W0,W1,J,cmap=\n",
        "  cm.coolwarm,linewidth=0, antialiased=False)\n",
        "\n",
        "  max_iter = 10000\n",
        "  clf.fit(X, y, learning_rate = .1, eps = 1e-5, max_iter = max_iter)\n",
        "  print(clf.w)\n",
        "  print(f'The accuracy of the logistic regression on the training set is {clf.score(X,y)}')\n",
        "  plt.subplots()\n",
        "  plot_1D_data(pos,neg,xlim=(-1,5),ylim=(-0.1,1))\n",
        "  plt.title(f'Iteration: {max_iter}')\n",
        "  plt.plot(xx,sigmoid(np.dot(x2,clf.w)))\n",
        "  plt.show()\n",
        "\n",
        "  # show error\n",
        "  plt.title('Convergence')\n",
        "  plt.plot(clf.training_error, label='error no shift')\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.grid(color=[.9,.9,.9])\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RPU0_tDAhwd7",
        "00c_N3r0hnyr"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}