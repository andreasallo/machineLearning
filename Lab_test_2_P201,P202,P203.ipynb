{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasallo/machineLearning/blob/main/Lab_test_2_P201%2CP202%2CP203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e774a6e-f147-4ee7-9b03-2050d0f65dab",
      "metadata": {
        "id": "6e774a6e-f147-4ee7-9b03-2050d0f65dab"
      },
      "source": [
        "# Machine Learning\n",
        "## **Lab Test 2** P201, P202, P203\n",
        "\n",
        "---\n",
        "\n",
        "Full Name:\n",
        "\n",
        "---\n",
        "\n",
        "U-Number:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8645fc-488e-4ee0-8668-08d0f1fec02d",
      "metadata": {
        "id": "3a8645fc-488e-4ee0-8668-08d0f1fec02d"
      },
      "source": [
        "## **Instructions**\n",
        "\n",
        "# Submission\n",
        "\n",
        "To submit your test, you need to print this file as a pdf and submit **both** the .ipynb and the .pdf to the Aula Global.\n",
        "\n",
        "**Use the task created for your group**.\n",
        "\n",
        "Please use the following file names:\n",
        "\n",
        "* `FULLNAME_UXXXXX_P3.pdf`\n",
        "* `FULLNAME_UXXXXX_P3.ipynb`\n",
        "\n",
        "Where FULLNAME is your full name and UXXXXX is your U-number.\n",
        "\n",
        "# Procedure\n",
        "\n",
        "Write the answers in the provided `code` cells.\n",
        "\n",
        "You must use as a basis the functions provided at the end of the questions. Feel free to edit them at your convenience. You cannot use ChatGPT or any other LLM. You can use notebooks from previous sessions of seminar and practical sessions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Consider the following dataset"
      ],
      "metadata": {
        "id": "T1h7WUOmSITh"
      },
      "id": "T1h7WUOmSITh"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[2, 2], [1, 2],[2, 1], [2, 3], [5, 1], [1, 3], [3,3], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0, 0, 1, 1, 0])"
      ],
      "metadata": {
        "id": "qXNc6obhSHki"
      },
      "id": "qXNc6obhSHki",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1:** Run gradient descent to optimize logistic linear classifier on the same dataset.\n",
        "1. What is the error (negative log likelihood) of the obtained solution?\n",
        "2. How many examples are classified correctly?\n",
        "\n",
        "**Important:** Use the following parameters: `eps = 0.0001, max_iter = 200, eta = 0.1, lam = 0`"
      ],
      "metadata": {
        "id": "EKcTYDgdcuY-"
      },
      "id": "EKcTYDgdcuY-"
    },
    {
      "cell_type": "code",
      "source": [
        "## ANSWER FOR QUESTION 1. Write your code here\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Reason about your answer here:"
      ],
      "metadata": {
        "id": "8jcTzUINbs4n"
      },
      "id": "8jcTzUINbs4n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 2:** Run the perceptron algorithm on the dataset. Does it converge?\n",
        "\n",
        "**Important:** Remember the class labels in the perceptron are encoded as `+1` and `-1`. You may find convenient to plot the dataset first (but you don't need to to answer this question). Report the *number of iterations until convergence* or alternatively *why do you think it does not converge*."
      ],
      "metadata": {
        "id": "x-svl0GMcY2d"
      },
      "id": "x-svl0GMcY2d"
    },
    {
      "cell_type": "code",
      "source": [
        "## ANSWER FOR QUESTION 2. Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Reason about your answer here:"
      ],
      "metadata": {
        "id": "W1Ee6Z_ibp7v"
      },
      "id": "W1Ee6Z_ibp7v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3:** Apply a polynomial feature map of order `k` on the data. For `k=3`, does the dataset become linearly separable? If yes, show it. If not, why not?"
      ],
      "metadata": {
        "id": "3cEkB4BKZCL9"
      },
      "id": "3cEkB4BKZCL9"
    },
    {
      "cell_type": "code",
      "source": [
        "## ANSWER FOR QUESTION 3. Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Reason about your answer here:"
      ],
      "metadata": {
        "id": "iX_pWeOXjzXJ"
      },
      "id": "iX_pWeOXjzXJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Question 4:** Can you achieve zero error using gradient descent to optimize logistic linear classifier on the extended dataset with polynomial features of order `k=3`? If yes, show it. If not, why not?"
      ],
      "metadata": {
        "id": "C3g0VsUDZilA"
      },
      "id": "C3g0VsUDZilA"
    },
    {
      "cell_type": "code",
      "source": [
        "## ANSWER FOR QUESTION 4. Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Reason about your answer here:"
      ],
      "metadata": {
        "id": "JTLRABn9aF06"
      },
      "id": "JTLRABn9aF06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the following cells freely. But remember to paste the final code (your answers in the previous cells)"
      ],
      "metadata": {
        "id": "5gFeTo0QcAhX"
      },
      "id": "5gFeTo0QcAhX"
    },
    {
      "cell_type": "code",
      "source": [
        "## imports\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "pkrpQUA1Z5-l"
      },
      "id": "pkrpQUA1Z5-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Perceptron class (feel free to change it if you need to)\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    An implementation for the original perceptron, a linear classification\n",
        "    algorithm for binary classification tasks.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    num_epochs: int\n",
        "        The number of epochs (iterations) to run the perceptron algorithm for.\n",
        "    weights: np.ndarray, shape depends on the dataset\n",
        "        The weight vector for the perceptron. Initialized to zeros.\n",
        "    training_error: np.ndarray, shape (num_epochs,)\n",
        "        Contains the number of mistakes made at every epoch.\n",
        "    weights_history: np.ndarray, shape depends on the dataset\n",
        "        Contains the weights computed at every epoch.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the perceptron to the training data (X, y) using the perceptron\n",
        "        algorithm. See method for a detailed description.\n",
        "\n",
        "    predict(X):\n",
        "        Predicts the class labels for the input data X using the current weight\n",
        "        vector. See method for a detailed description.\n",
        "\n",
        "    plot_training_error():\n",
        "        Plot the training error. The x-axis corresponds to the epoch, the y-axis\n",
        "        corresponds to the number of mistakes made within an epoch.\n",
        "\n",
        "    plot_decision_boundary(X, y, weights_to_plot):\n",
        "        Plot the dataset (X, y) and the decision boundary of the classifier with\n",
        "        weights from specific epochs indicated by `weights_to_plot`. See method\n",
        "        for a detailed description.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_epochs=10):\n",
        "        \"\"\"\n",
        "        Initializes a new Perceptron instance.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        num_epochs: int, optional (default=10)\n",
        "            The number of epochs to run the perceptron algorithm for.\n",
        "        \"\"\"\n",
        "        self.num_epochs = num_epochs\n",
        "        self.training_error = np.zeros(num_epochs)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the perceptron to the training data (X, y) using the perceptron\n",
        "        algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X: np.ndarray, shape (num_samples, num_features)\n",
        "            The training data matrix, where each row is an input vector and each\n",
        "            column is a feature.\n",
        "        y: np.ndarray, shape (num_samples,)\n",
        "            The class label vector for the training data. Each element\n",
        "            corresponds to the class label for the corresponding row in X.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self: Perceptron\n",
        "            The instance that was trained on the dataset (X, y).\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # For simplicity, add a column of ones to the features to include the\n",
        "        # bias in the weight vector\n",
        "        column_ones = np.ones((num_samples, 1))\n",
        "        X_aug = np.hstack((column_ones, X))\n",
        "\n",
        "        # Initialize weight vector to zeros\n",
        "        self.weights = np.zeros(num_features + 1)\n",
        "        self.weights_history = np.zeros((self.num_epochs, num_features + 1))\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "\n",
        "          # Log the weights of the current epoch\n",
        "          self.weights_history[epoch, :] = self.weights\n",
        "          count_errors = 0\n",
        "          for i in range(num_samples):\n",
        "\n",
        "            # Select the i-th sample\n",
        "            x = X_aug[i, :]\n",
        "            y_true = y[i]\n",
        "\n",
        "            # Calculate dot product of weights and input vector\n",
        "            z = np.dot(self.weights, x)\n",
        "\n",
        "            # Update every other mistake\n",
        "            if y_true * z <= 0:\n",
        "              count_errors += 1\n",
        "              self.weights += y_true * X_aug[i, :]\n",
        "\n",
        "          # Log the errors of the current epoch\n",
        "          self.training_error[epoch] = count_errors\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the class labels for the input data X using the current weight\n",
        "        vector.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X: numpy.ndarray of shape (num_samples, num_features)\n",
        "            The input data matrix, where each row is an input vector and each\n",
        "            column is a feature.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        y_pred: numpy.ndarray of shape (num_samples,)\n",
        "            The predicted class labels for the input data, where each element is\n",
        "            either 1 or -1.\n",
        "        \"\"\"\n",
        "        num_samples = X.shape[0]\n",
        "        y_pred = np.zeros(num_samples)\n",
        "\n",
        "        # Add a column of ones for the bias\n",
        "        column_ones = np.ones((num_samples, 1))\n",
        "        X_aug = np.hstack((column_ones, X))\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Calculate dot product of weights and input vector\n",
        "            z = np.dot(self.weights, X_aug[i, :])\n",
        "\n",
        "            # Apply step function to activation to get predicted class\n",
        "            y_pred[i] = np.where(z >= 0, 1, -1)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def plot_training_error(self):\n",
        "        \"\"\"\n",
        "        Plot the training error. The x-axis corresponds to the epoch, the y-axis\n",
        "        corresponds to the number of mistakes made within an epoch.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(5, 3))\n",
        "        plt.title('Training error')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Number of errors')\n",
        "        plt.plot(self.training_error)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_decision_boundary(self, X, y, weights_to_plot=None):\n",
        "        \"\"\"\n",
        "        Plot the training error. The x-axis corresponds to the epoch, the y-axis\n",
        "        corresponds to the number of mistakes made within an epoch.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X: np.ndarray, shape (num_samples, num_features)\n",
        "            An input data matrix, where each row is an input vector and each\n",
        "            column is a feature.\n",
        "\n",
        "        y: np.ndarray, shape (num_samples,)\n",
        "            The class label vector for the training data. Each element\n",
        "            corresponds to the class label for the corresponding row in X.\n",
        "\n",
        "        weights_to_plot: np.ndarray, shape (num_weights,)\n",
        "            A list of epochs indices, to indicate which epochs to plot.\n",
        "        \"\"\"\n",
        "        if weights_to_plot is None:\n",
        "          weights_to_plot = np.array([-1])  # Plot the last decision boundary\n",
        "\n",
        "        num_weights = len(weights_to_plot)\n",
        "        print(weights_to_plot)\n",
        "\n",
        "        plt.figure()\n",
        "        colormap = ListedColormap(np.array(plt.cm.Paired.colors)[[1, 5]])\n",
        "\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colormap, linewidth=.5)\n",
        "        plt.scatter([], [], color=colormap.colors[0], linewidth=.5, label='Class -1')\n",
        "        plt.scatter([], [], color=colormap.colors[1], linewidth=.5, label='Class 1')\n",
        "\n",
        "        xlim = plt.xlim()\n",
        "        ylim = plt.ylim()\n",
        "\n",
        "        xmin = xlim[0] - 0.1 * abs(xlim[0])\n",
        "        xmax = xlim[1] + 0.1 * abs(xlim[1])\n",
        "\n",
        "        ymin = ylim[0] - 0.1 * abs(ylim[0])\n",
        "        ymax = ylim[1] + 0.1 * abs(ylim[1])\n",
        "\n",
        "        xx = np.linspace(xmin, xmax, 10)\n",
        "\n",
        "        viridis = plt.colormaps['viridis']\n",
        "        colors = [viridis(i) for i in np.linspace(0, 1, num_weights)]\n",
        "\n",
        "        for i, j in enumerate(weights_to_plot):\n",
        "            weights = self.weights_history[j, :]\n",
        "            if weights[2] != 0:\n",
        "                yy = - (weights[1] * xx + weights[0]) / weights[2]\n",
        "                plt.plot(xx, yy, c=colors[i], label=f'Epoch {j}')\n",
        "            elif weights[1] != 0:\n",
        "                plt.axvline(x = - weights[0] / weights[1], color=colors[i], label=f'Epoch {j}')\n",
        "\n",
        "        plt.xlim((xmin, xmax))\n",
        "        plt.ylim((ymin, ymax))\n",
        "        plt.title('Decision boundary')\n",
        "        plt.xlabel('x_1')\n",
        "        plt.ylabel('x_2')\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "CWGeJFcVcDfi"
      },
      "id": "CWGeJFcVcDfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42985af-b69d-49e2-b7b9-4ef3e6bd70a6",
      "metadata": {
        "id": "b42985af-b69d-49e2-b7b9-4ef3e6bd70a6"
      },
      "outputs": [],
      "source": [
        "## Gradient descent for Linear Logistic Classifier (feel free to change it if you need to)\n",
        "class my_GDClassifier():\n",
        "    \"\"\"\n",
        "    Logistic Regression implemented with Gradient Descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, add_bias = True, lam = 0, max_iter = 1000, eps = 1e-8, eta = 1e-2):\n",
        "        \"\"\"\n",
        "        Creates an instance of our my_GDClassifier model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_iter: maximun number of iteration if convergence is not achieved.\n",
        "        eps: sets the convergence criterion.\n",
        "        eta: learning rate or step size.\n",
        "        lam: weight controlling the regularizer, or lambda.\n",
        "        \"\"\"\n",
        "        self.add_bias = add_bias\n",
        "        self.lam = lam\n",
        "        self.max_iter = max_iter\n",
        "        self.eps = eps\n",
        "        self.eta = eta\n",
        "        self.nb_iter = 1\n",
        "        self.training_error = np.array([])\n",
        "        self.n_samples = 0\n",
        "        self.n_features = 0\n",
        "        if add_bias:\n",
        "          self.w = np.zeros([1, self.n_features + 1])[0]\n",
        "        else:\n",
        "          self.w = np.zeros([1, self.n_features])[0]\n",
        "\n",
        "    def cross_entropy_error(self, X, y, w):\n",
        "        \"\"\"\n",
        "        Computes the Cross-Entropy Loss Function\n",
        "        Assumes bias is included in w\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "\n",
        "        w: ndarray, shape(1, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        error: integer,\n",
        "                The Cross-Entropy Loss\n",
        "        \"\"\"\n",
        "        loglik = 0\n",
        "        for n in range(X.shape[0]):\n",
        "            yhat = sigmoid(w@X[n,:])\n",
        "            loglik_n = y[n]*np.log(yhat+1e-9)+(1-y[n])*np.log(1-yhat+1e-9)\n",
        "            # Note: the 1e-9 is added to avoid numerical errors\n",
        "\n",
        "            loglik += loglik_n\n",
        "\n",
        "        return -loglik/X.shape[0] + self.lam/2 * np.power(np.linalg.norm(w),2)\n",
        "\n",
        "    def gradient(self, X, y, w):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the objective function at point w using data set (X, y)\n",
        "        Assumes Cross-Entropy and binary logistic regression model\n",
        "        Assumes bias is included in w\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "\n",
        "        w: ndarray, shape(1, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        grad: ndarray, shape (1, n_features)\n",
        "                The gradient of the Cross-Entropy Loss Function\n",
        "        \"\"\"\n",
        "\n",
        "        #initialize gradient\n",
        "        if self.add_bias:\n",
        "          grad = np.zeros([self.n_features+1])\n",
        "        else:\n",
        "          grad = np.zeros([self.n_features])\n",
        "\n",
        "        for n in range(X.shape[0]):\n",
        "            yhat = sigmoid(w@X[n,:])\n",
        "\n",
        "            gradient_n = (yhat-y[n])*X[n,:]\n",
        "            grad += gradient_n\n",
        "\n",
        "        return grad/X.shape[0] + self.lam*w\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit our Logistic Regression model to the data set (X, y)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self: logReg\n",
        "               The Logistic Regression model trained on (X, y).\n",
        "        \"\"\"\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.n_features = X.shape[1]\n",
        "        if self.add_bias:\n",
        "          X = np.hstack([np.ones((self.n_samples,1)),X])  # add dummy input\n",
        "          self.w = np.random.randn(self.n_features + 1)\n",
        "        else:\n",
        "          self.w = np.random.randn(self.n_features)\n",
        "\n",
        "        convergence = False  # start with cond greater than eps\n",
        "        prev_error = np.Inf\n",
        "        while (not convergence) and (self.nb_iter < self.max_iter):\n",
        "            w_old = self.w\n",
        "\n",
        "            #update weights\n",
        "            self.w = w_old - self.eta*self.gradient(X, y, self.w)\n",
        "\n",
        "            #compute cross entropy error for this iteration and save it\n",
        "            error = self.cross_entropy_error(X, y, self.w)\n",
        "\n",
        "            self.training_error = np.append(self.training_error, error)\n",
        "            self.nb_iter += 1\n",
        "\n",
        "            #check convergence condition\n",
        "            convergence = abs(error - prev_error) < self.eps\n",
        "            prev_error = error\n",
        "\n",
        "        print(f'CE Error {error}')\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The set of points whose classes are to predict.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions: ndarray, shape (n_samples)\n",
        "                      The predicted classes.\n",
        "                      n_samples == number of points in the dataset.\n",
        "        \"\"\"\n",
        "        if self.add_bias:\n",
        "          X = np.hstack([np.ones((X.shape[0],1)),X])\n",
        "\n",
        "        # return prediction of the model for given X data points\n",
        "        # the returned type is integer taking values {0,1}\n",
        "        prediction = np.reshape((sigmoid(self.w@X.T) > 0.5).astype(int),(-1,))\n",
        "        return prediction\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the classifier on the set X, provided the ground-truth y.\n",
        "        Assumes the representation of the y is {0,1}\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The set of points on which to compute the score.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The ground-truth of the labels.\n",
        "            n_samples == number of points in the dataset.\n",
        "        Returns\n",
        "        -------\n",
        "        score: float\n",
        "                Accuracy of the classifier.\n",
        "        \"\"\"\n",
        "        if self.training_error.size == 0:\n",
        "          print('train first!')\n",
        "          return -1\n",
        "        else:\n",
        "          return np.mean(self.predict(X) == y)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:crystal_clear]",
      "language": "python",
      "name": "conda-env-crystal_clear-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}